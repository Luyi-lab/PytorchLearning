{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD\n",
    "#back propagation is used to train network\n",
    "#parameters are adjusted accorting to the gradient of loss\n",
    "#with respect to the gicen parameter\n",
    "\n",
    "# built-in differentiation engine called torch.autograd. \n",
    "# It supports automatic computation of gradient for \n",
    "# any computational graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the simplest one-layer neural network\n",
    "# input x parameters w and b, and loss\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5) #input tensor\n",
    "y = torch.zeros(3) #expected output\n",
    "\n",
    "#parameters, which we need to optimize\n",
    "#so set repuires_grad True\n",
    "#Note:can set the value of requires_grad when creating a tensor\n",
    "# or later by using x.requires_grad_(True) method.\n",
    "w = torch.randn(5,3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "#y hat\n",
    "z = torch.matmul(x,w)+b\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7f01f45c2f90>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f01f45c2f50>\n"
     ]
    }
   ],
   "source": [
    "#what we define in code is an object of class Function\n",
    "#The backward propagation functon is stored in \n",
    "# grad_fn property of a tensor\n",
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2241, 0.1198, 0.0173],\n",
      "        [0.2241, 0.1198, 0.0173],\n",
      "        [0.2241, 0.1198, 0.0173],\n",
      "        [0.2241, 0.1198, 0.0173],\n",
      "        [0.2241, 0.1198, 0.0173]])\n",
      "tensor([0.2241, 0.1198, 0.0173])\n"
     ]
    }
   ],
   "source": [
    "#use loss.backwrd() to compute derivatives\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "#Note_1:can only obtain the grad properties for \n",
    "# the leaf nodes of the computational graph,\n",
    "# which have requires_grad property set to True\n",
    "\n",
    "#Note_2: can only perform gradient calculations\n",
    "# using backward once on a given graph, for performance reasons\n",
    "# If we need to do several backward calls on the same graph, \n",
    "# we need to pass retain_graph=True to the backward call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Default tensor with requires_grad=True are \n",
    "# tracking their computational history and support gradient computation. \n",
    "# when we have trained the model and just want to apply it to some input data, \n",
    "# i.e. we only want to do forward computations through the network. \n",
    "\n",
    "# We can stop tracking computations by \n",
    "# surrounding our computation code with torch.no_grad() block\n",
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "#Another way is to use detach() on the tensor\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Computational Graph\n",
    "Autograd keeps a record of data (tensors) and \n",
    "\n",
    "all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects\n",
    "\n",
    "In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "Forward pass, autograd does two things simultaneously:\n",
    "\n",
    "run the requested operation to compute a resulting tensor\n",
    "\n",
    "maintain the operation’s gradient function in the DAG.\n",
    "\n",
    "The backward pass kicks off when .backward() is called on the DAG root. autograd then:\n",
    "\n",
    "computes the gradients from each .grad_fn,\n",
    "\n",
    "accumulates them in the respective tensor’s .grad attribute\n",
    "\n",
    "using the chain rule, propagates all the way to the leaf tensors.\n",
    "    \n",
    " after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]], requires_grad=True)\n",
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "#Tensor Gradients and Jacobian Products\n",
    "# the output function is an arbitrary tensor. In this case,\n",
    "# PyTorch allows you to compute so-called Jacobian product, \n",
    "# and not the actual gradient.\n",
    "\n",
    "#Instead of computing the Jacobian matrix itself,\n",
    "# PyTorch allows you to compute Jacobian Product \n",
    "# Achieved by backward with v as an argument. v is the var being respected\n",
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)\n",
    "#If you want to compute the proper gradients, you need to zero out the grad property before. \n",
    "# In real-life training an optimizer helps us to do this.\n",
    "#Note:Previously we were calling backward() function without parameters. \n",
    "# This is essentially equivalent to calling backward(torch.tensor(1.0)), \n",
    "# which is a useful way to compute the gradients in case of a scalar-valued function, \n",
    "# such as loss during neural network training."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d80b2155b04d7438a6bcae156c925d5937454b2981d3e8c144c422ccbfcad5a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('luyi_py37_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
